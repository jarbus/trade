---- Notes on Collect-Then-Trade ----


axioms: live as long as possible <--- when should there be death?
- when any of the resources needed are not met?
- when some of the resources needed are not met?
live as good of a life as possible, die if future looks bleak?
be social
don't expend too much energy
don't starve

Experiment goals:
single beneficial interaction (collect -> trade)
repeated beneficial interactions (collect -> trade -> collect -> trade)

1D: food at edge of map, agents go to collect it, then trade

Switched: Now using trade_v3, where agents will start out with
1 of each food, and rest of food will need to be collected
reward function: sum(inv_dist(a, agent) for a in agents)

Potential ideas:
    apply movement penalty to incentivize not walking across the entire map

--- Experiments ---

(1, 7):
    Board f.0.1.F
    fails, agents just move close together
death_prob_increases_with_hunger (1, 7):
    There's currently no benefit for getting just one resource, so making death probabilistic
    where the probability of death increases as people get hungry might incentivize having more of each resource
    This fails. Agents might learn to move close together, but generally don't learn to trade.
    Also tried with larger batch size, which proved less variable

Changed: Only metabolise food when there is food, agents won't go (too far) below 0
Changed: Permutation interval for PBT 40 -> 100

On the subject of searching different reward functions:
    sweep parameters for movement, distance, and death probability params
    question: should agents lose food for moving, or just reward?

Experiment: reward_sweep:
    Will different reward hyperparameters yield trading behavior?
    sweep parameters for movement, distance, and death probability params
    write visualizer to make videos of sample runs
    
    Result: Need more steps, 4m is not enough, it looked like it was going to increase for some runs at 4m steps

Added: metric for how much food agents picked and placed
Today: Run with and without communication

Hypothesis: Agents that can communicate can directly influence other agents, communication "can" make exchange emerge faster. Agents might also learn to ignore comms if they get exploited too much. How to enforce cooperative comms?
- Unlikely, but worth testing

Experiment: 10m steps:
    agents get close to trading, but not quite. One run picks up food 0, then doesn't go after it and explores other options, then goes back to picking it up, but learns to drop it for the other agent to pick up now. I think there is an insane amount of exploration going on here that has tons of different dimensions, and we might need even longer time horizons. That being said, many runs didn't display any exchange behavior at all, only 3 out of the 16 did. I need to make a note of which reward settings were successful in trading at least one resource. Furthermore, I think agents will first need to learn to exchange one resource, then another: having two people learn to exchange simultaneously simply doesn't work.

Experiment: 20m steps:
Experiment: 10m steps with communication:

Date: 05-28-22

NOTES: ONE EXPERIMENT DID TRADE
- figure out why it worked
- figure out evolution of behavior

Three trials eventually learned some aspect of trade behavior, indicated by a significantly increased lifetime

--- RESULTS ---

trades both: vocab=0 7b970_00014_14_clip_param=0.1,entropy_coeff=0.01,death_prob=0.1,dist_coeff=0.1,move_coeff=0.0
trades both: vocab=0 7b970_00005_5_clip_param=0.1,entropy_coeff=0.01,death_prob=0.05,dist_coeff=0.1,move_coeff=0.0
trades both: vocab=0 7b970_00002_2_clip_param=0.2,entropy_coeff=0.1,death_prob=0.15,dist_coeff=0.2,move_coeff=0.1

trades f1: vocab=8 9cc03_00000_0_clip_param=0.05,entropy_coeff=0.01,death_prob=0.1,dist_coeff=0.2,move_coeff=0.1
trades f1: vocab=8 9cc03_00003_3_clip_param=0.1,entropy_coeff=0.01,death_prob=0.1,dist_coeff=0.5,move_coeff=0.1
trades f1: vocab=0 3da8e_00000_0_clip_param=0.2,entropy_coeff=0.05,death_prob=0.05,dist_coeff=0.2,move_coeff=0.2
trades f1: vocab=0 3da8e_00002_2_clip_param=0.05,entropy_coeff=0.05,death_prob=0.1,dist_coeff=0.5,move_coeff=0.1
trades f1: vocab=0 7b970_00003_3_clip_param=0.1,entropy_coeff=0.01,death_prob=0.15,dist_coeff=0.5,move_coeff=0.1

trades f0: vocab=0 3da8e_00001_1_clip_param=0.1,entropy_coeff=0.01,death_prob=0.15,dist_coeff=1.0,move_coeff=0.0

            |     3d8ae     |      7b970     |     9cc03
            |  Vocab 0 10m  |   Vocab 0 20m  |  Vocab 8 10m
Trades Both |            0  |             3  |           0
Trades 1    |            2  |             1  |           2
Trades 0    |            1  |             1  |           0
No Trades   |           13  |            11  |          14


--- Parameter Ranges ---
Entropy: 0.01, 0.05, 0.1
Clip:    0.05, 0.10, 0.2
Death:   0.05, 0.10, 0.15, 0.20, 0.5
Dist:    0.00, 0.10, 0.20, 0.50, 1.0
Move:    0.00, 0.10, 0.20, 0.30
SGD:     5, 10, 15

Notes on Parameters:
    - Distance coefficient seems best around 0.1, but must be positive
    - Death prob needs to be low, around 0.1

Notes on timesteps:
    - 20 million steps vocab 0 worked better than 10m steps but reached a solution before 10m
    - I think I disabled pbt properly for this one

Can we get this to come out faster?
    - Better learning rate?
    - Varied batch size?
    - Random environment starts?
    - Figure out conditions this behavior can emerge in, and see if this can scale to 2d
    - Change step order to be random
    - Yes

Changed: Action order is random

Experiment: Speedup (vary learning Rate)
    batch size 4000
    lr = 0.001 and 0.0001  results in very fast convergence
    move_coeff = 0.0 seems to yield far more successes than move_coeff = 0.1
    num_sgd_iter=5 seems to be enough
    Experiment with smaller batch sizes

Experiment: Lower batch size
    Batch size 1000 is stable and emerges trade across the board
Experiment: Respawn food (5m)
    agents learn to keep trading but trading takes longer to emerge
    batch size 2k
    high variance, agents never converge properly
Experiment: Respawn food after 20 steps (10m) (bs=1k)
    high variance, agents never converge properly, probably need to adjust learning rate later on
    i think agents adjust between fully exchanging and barely exchanging at all, for some reason they can't learn later on without changing their prior behavior?
    Maybe I should focus on scaing this to a small series of exchanges after food is exhausted in 2d, rather than iterated exchange in 1d

